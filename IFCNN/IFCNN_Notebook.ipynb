{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for running IFCNN to fuse multiple types of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project page of IFCNN is https://github.com/uzeful/IFCNN.\n",
    "\n",
    "If you find this code is useful for your research, please consider to cite our paper.\n",
    "```\n",
    "@article{zhang2019IFCNN,\n",
    "  title={IFCNN: A General Image Fusion Framework Based on Convolutional Neural Network},\n",
    "  author={Zhang, Yu and Liu, Yu and Sun, Peng and Yan, Han and Zhao, Xiaolin and Zhang, Li},\n",
    "  journal={Information Fusion},\n",
    "  volume={54},\n",
    "  pages={99--118},\n",
    "  year={2020},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```\n",
    "\n",
    "Detailed procedures to use IFCNN are introduced as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from model import myIFCNN\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from utils.myTransforms import denorm, norms, detransformcv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the well-trained image fusion model (IFCNN-MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use fuse_scheme to choose the corresponding model, \n",
    "# choose 0 (IFCNN-MAX) for fusing multi-focus, infrare-visual and multi-modal medical images, 2 (IFCNN-MEAN) for fusing multi-exposure images\n",
    "fuse_scheme = 0\n",
    "if fuse_scheme == 0:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "elif fuse_scheme == 1:\n",
    "    model_name = 'IFCNN-SUM'\n",
    "elif fuse_scheme == 2:\n",
    "    model_name = 'IFCNN-MEAN'\n",
    "else:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "\n",
    "# load pretrained model\n",
    "model = myIFCNN(fuse_scheme=fuse_scheme)\n",
    "model.load_state_dict(torch.load('snapshots/'+ model_name + '.pth'))\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use IFCNN to respectively fuse CMF, IV and MD datasets\n",
    "Fusion images are saved in the 'results' folder under your current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time of CMF dataset: 11.9s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/IVDataset/Camp_Vis.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bb89e8d85568>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m                                       \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                                   ]))\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mimg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mimg2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\document\\Study\\Code\\其他代码\\IFCNN\\utils\\myDatasets.py\u001b[0m in \u001b[0;36mget_pair\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_image_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpath1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mimg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpath1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_image_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpath2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mimg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpath2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\document\\Study\\Code\\其他代码\\IFCNN\\utils\\myDatasets.py\u001b[0m in \u001b[0;36mloader\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mis_image_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2809\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/IVDataset/Camp_Vis.png'"
     ]
    }
   ],
   "source": [
    "from utils.myDatasets import ImagePair\n",
    "\n",
    "IV_filenames = ['Camp', 'Camp1', 'Dune', 'Gun', 'Navi', 'Kayak', 'Octec', 'Road', 'Road2', 'Steamboat', 'T2', 'T3', 'Trees4906', 'Trees4917', 'Window']\n",
    "MF_filenames = ['clock',  'lab', 'pepsi', 'book', 'flower', 'desk', 'seascape', 'temple', 'leopard', 'wine', 'balloon', 'calendar', 'corner', 'craft', 'leaf', 'newspaper', 'girl', 'grass', 'toy']\n",
    "\n",
    "datasets = [ 'IV'] # Color MultiFocus, Infrared-Visual, MeDical image datasets\n",
    "datasets_num = [20, 15, 8]     # number of image sets in each dataset\n",
    "is_save = True                 # if you do not want to save images, then change its value to False\n",
    "\n",
    "for j in range(1):\n",
    "    j=1\n",
    "    begin_time = time.time()\n",
    "    for ind in range(50):\n",
    "        if j == 0:\n",
    "            # lytro-dataset: two images. Number: 20\n",
    "            dataset = datasets[j]      # Color Multifocus Images\n",
    "            is_gray = False            # Color (False) or Gray (True)\n",
    "            mean=[0.485, 0.456, 0.406] # normalization parameters\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "            \n",
    "            root = 'datasets/CMFDataset/'\n",
    "            filename = 'lytro-{:02}'.format(ind+1)\n",
    "            path1 = os.path.join('{0}-A.jpg'.format(root + filename))\n",
    "            path2 = os.path.join('{0}-B.jpg'.format(root + filename))\n",
    "        elif j == 1:\n",
    "            # infrare and visual image dataset. Number: 14\n",
    "            dataset = datasets[j]  # Infrared and Visual Images\n",
    "            is_gray = True         # Color (False) or Gray (True)\n",
    "            mean=[0, 0, 0]         # normalization parameters\n",
    "            std=[1, 1, 1]\n",
    "            \n",
    "            root = 'datasets/IVDataset/'\n",
    "            filename = IV_filenames[ind]\n",
    "            path1 = \"../road/vi/%d.jpg\"%ind\n",
    "            path2 = \"../road/ir/%d.jpg\"%ind\n",
    "        elif j == 2:\n",
    "            # medical image dataset: CT (MR) and MR. Number: 8\n",
    "            dataset = datasets[j]  # Medical Image\n",
    "            is_gray = True         # Color (False) or Gray (True)\n",
    "            mean=[0, 0, 0]         # normalization parameters\n",
    "            std=[1, 1, 1]\n",
    "            \n",
    "            root = 'datasets/MDDataset/'\n",
    "            filename = 'c{:02}'.format(ind+1)\n",
    "            path1 = os.path.join('{0}_1.tif'.format(root + filename))\n",
    "            path2 = os.path.join('{0}_2.tif'.format(root + filename))\n",
    "\n",
    "        \n",
    "        # load source images\n",
    "        pair_loader = ImagePair(impath1=path1, impath2=path2, \n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean, std=std)\n",
    "                                  ]))\n",
    "        img1, img2 = pair_loader.get_pair()\n",
    "        img1.unsqueeze_(0)\n",
    "        img2.unsqueeze_(0)\n",
    "\n",
    "        # perform image fusion\n",
    "        with torch.no_grad():\n",
    "            res = model(Variable(img1.cuda()), Variable(img2.cuda()))\n",
    "            res = denorm(mean, std, res[0]).clamp(0, 1) * 255\n",
    "            res_img = res.cpu().data.numpy().astype('uint8')\n",
    "            img = res_img.transpose([1,2,0])\n",
    "\n",
    "        # save fused images\n",
    "        if is_save:\n",
    "            filename = model_name + '-' + dataset + '-' + filename\n",
    "            if is_gray:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                img = Image.fromarray(img)\n",
    "                img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "            else:\n",
    "                img = Image.fromarray(img)\n",
    "                img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "\n",
    "    # when evluating time costs, remember to stop writing images by setting is_save = False\n",
    "    proc_time = time.time() - begin_time\n",
    "    print('Total processing time of {} dataset: {:.3}s'.format(datasets[j], proc_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use IFCNN to fuse triple multi-focus images in CMF dataset\n",
    "Fusion images are saved in the 'results' folder under your current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time of CMF3 dataset: 0.223s\n"
     ]
    }
   ],
   "source": [
    "from utils.myDatasets import ImageSequence\n",
    "\n",
    "dataset = 'CMF3'           # Triple Color MultiFocus\n",
    "is_save = True             # Whether to save the results\n",
    "is_gray = False            # Color (False) or Gray (True)\n",
    "is_folder=False            # one parameter in ImageSequence\n",
    "mean=[0.485, 0.456, 0.406] # Color (False) or Gray (True)\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "begin_time = time.time()\n",
    "for ind in range(4):\n",
    "    # load the sequential source images\n",
    "    root = 'datasets/CMFDataset/Triple Series/'\n",
    "    filename = 'lytro-{:02}'.format(ind+1)\n",
    "    paths = []\n",
    "    paths.append(os.path.join('{0}-A.jpg'.format(root + filename)))\n",
    "    paths.append(os.path.join('{0}-B.jpg'.format(root + filename)))\n",
    "    paths.append(os.path.join('{0}-C.jpg'.format(root + filename)))\n",
    "    filename = model_name + '-' + dataset + '-' + 'lytro-{:02}'.format(ind+1)\n",
    "\n",
    "    seq_loader = ImageSequence(is_folder, 'RGB', transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=mean, std=std),\n",
    "                                ]), *paths)\n",
    "    imgs = seq_loader.get_imseq()\n",
    "    \n",
    "    # perform image fusion\n",
    "    with torch.no_grad():\n",
    "        vimgs = []\n",
    "        for idx, img in enumerate(imgs):\n",
    "            img.unsqueeze_(0)\n",
    "            vimgs.append(Variable(img.cuda()))\n",
    "        vres = model(*vimgs)\n",
    "        res = denorm(mean, std, vres[0]).clamp(0, 1) * 255\n",
    "        res_img = res.cpu().data.numpy().astype('uint8')\n",
    "        img = Image.fromarray(res_img.transpose([1,2,0]))\n",
    "    \n",
    "    # save the fused image\n",
    "    if is_save:\n",
    "        if is_gray:\n",
    "            img.convert('L').save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "        else:\n",
    "            img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "            \n",
    "# when evluating time costs, remember to stop writing images by setting is_save = False\n",
    "proc_time = time.time() - begin_time\n",
    "print('Total processing time of {} dataset: {:.3}s'.format(dataset ,proc_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load the well-trained image fusion model (IFCNN-MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use fuse_scheme to choose the corresponding model, \n",
    "# choose 0 (IFCNN-MAX) for fusing multi-focus, infrare-visual and multi-modal medical images, 2 (IFCNN-MEAN) for fusing multi-exposure images\n",
    "fuse_scheme = 2\n",
    "if fuse_scheme == 0:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "elif fuse_scheme == 1:\n",
    "    model_name = 'IFCNN-SUM'\n",
    "elif fuse_scheme == 2:\n",
    "    model_name = 'IFCNN-MEAN'\n",
    "else:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "\n",
    "# load pretrained model\n",
    "model = myIFCNN(fuse_scheme=fuse_scheme)\n",
    "model.load_state_dict(torch.load('snapshots/'+ model_name + '.pth'))\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use IFCNN to fuse various number of multi-exposure images in ME Dataset\n",
    "Fusion images are saved in the 'results' folder under your current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time of ME dataset: 0.32s\n"
     ]
    }
   ],
   "source": [
    "from utils.myDatasets import ImageSequence\n",
    "\n",
    "dataset = 'ME'\n",
    "is_save = True\n",
    "is_gray = False\n",
    "is_folder = True\n",
    "toggle = True\n",
    "is_save_Y = False\n",
    "mean=[0, 0, 0]\n",
    "std=[1, 1, 1]\n",
    "begin_time = time.time()\n",
    "root = 'datasets/MEDataset/'\n",
    "\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "    if toggle:\n",
    "        toggle = False\n",
    "    else:\n",
    "       # Load the sequential images in each subfolder\n",
    "        paths = [subdir]\n",
    "        seq_loader = ImageSequence(is_folder, 'YCbCr', transforms.Compose([\n",
    "                                      transforms.ToTensor()]), *paths)\n",
    "        imgs = seq_loader.get_imseq()\n",
    "\n",
    "        # separate the image channels\n",
    "        NUM = len(imgs)\n",
    "        c, h, w = imgs[0].size()\n",
    "        Cbs = torch.zeros(NUM, h, w)\n",
    "        Crs = torch.zeros(NUM, h, w)\n",
    "        Ys = []\n",
    "        for idx, img in enumerate(imgs):\n",
    "                #print(img)\n",
    "                Cbs[idx,:,:] = img[1]\n",
    "                Crs[idx,:,:] = img[2]\n",
    "                Ys.append(img[0].unsqueeze_(0).unsqueeze_(0).repeat(1,3,1,1)) #Y\n",
    "\n",
    "        # Fuse the color channels (Cb and Cr) of the image sequence\n",
    "        Cbs *= 255\n",
    "        Crs *= 255\n",
    "        Cb128 = abs(Cbs - 128);\n",
    "        Cr128 = abs(Crs - 128);\n",
    "        CbNew = sum((Cbs * Cb128) / (sum(Cb128).repeat(NUM, 1, 1)));\n",
    "        CrNew = sum((Crs * Cr128) / (sum(Cr128).repeat(NUM, 1, 1)));\n",
    "        CbNew[torch.isnan(CbNew)] = 128\n",
    "        CrNew[torch.isnan(CrNew)] = 128\n",
    "\n",
    "        # Fuse the Y channel of the image sequence\n",
    "        imgs = norms(mean, std, *Ys) # normalize the Y channels\n",
    "        with torch.no_grad():\n",
    "            vimgs = []\n",
    "            for idx, img in enumerate(imgs):\n",
    "                vimgs.append(Variable(img.cuda()))\n",
    "            vres = model(*vimgs)\n",
    "\n",
    "        # Enhance the Y channel using CLAHE\n",
    "        img = detransformcv2(vres[0], mean, std)                    # denormalize the fused Y channel\n",
    "        y = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)                   # generate the single y channel\n",
    "        \n",
    "        y = y / 255                                                 # initial enhancement\n",
    "        y = y * 235 + (1-y) * 16;\n",
    "        y = y.astype('uint8')\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))  # clahe enhancement\n",
    "        cy = clahe.apply(y)\n",
    "        \n",
    "        # Merge the YCbCr channels back and covert to RGB color space\n",
    "        cyrb = np.zeros([h,w,c]).astype('uint8')\n",
    "        cyrb[:,:,0] = cy\n",
    "        cyrb[:,:,1] = CrNew\n",
    "        cyrb[:,:,2] = CbNew\n",
    "        rgb = cv2.cvtColor(cyrb, cv2.COLOR_YCrCb2RGB)\n",
    "        \n",
    "        \n",
    "        # Save the fused image\n",
    "        img = Image.fromarray(rgb)\n",
    "        filename = subdir.split('/')[-1]\n",
    "        filename = model_name + '-' + dataset + '-' + filename  # y channels are fused by IFCNN, cr and cb are weighted fused\n",
    "        \n",
    "        if is_save:\n",
    "            if is_gray:\n",
    "                img.convert('L').save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "            else:\n",
    "                img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "\n",
    "# when evluating time costs, remember to stop writing images by setting is_save = False\n",
    "proc_time = time.time() - begin_time\n",
    "print('Total processing time of {} dataset: {:.3}s'.format(dataset, proc_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
